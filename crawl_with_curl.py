import json
import subprocess
import time
from pathlib import Path
import requests
from datetime import datetime, timedelta
import logging
import re 
from glob import glob

# --- ì„¤ì • ê°’ ---
BASE_DOWNLOAD_DIR = Path("/app/downloads") # ê¸°ë³¸ ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬
ONION_LIST_PATH = BASE_DOWNLOAD_DIR / "onion_list.json"
TOR_PROXY_ADDRESS = "127.0.0.1:9050"
MAX_PAGES_PER_FQDN = 5
CURL_CONNECT_TIMEOUT = 10  # curl ì—°ê²° íƒ€ì„ì•„ì›ƒ
CURL_MAX_TIME = 30         # curl ì „ì²´ ìµœëŒ€ ì‹¤í–‰ ì‹œê°„
SUBPROCESS_TIMEOUT = CURL_MAX_TIME + 10 # subprocessì˜ íƒ€ì„ì•„ì›ƒ (curl max_time ë³´ë‹¤ ê¸¸ê²Œ)
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; rv:109.0) Gecko/20100101 Firefox/115.0" # ì˜ˆì‹œ User-Agent

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def clean_old_group_files(group, today_str):
    """í•´ë‹¹ ê·¸ë£¹ í´ë”ì—ì„œ ì˜¤ëŠ˜ ë‚ ì§œê°€ ì•„ë‹Œ HTML íŒŒì¼ ì‚­ì œ"""
    group_dir = BASE_DOWNLOAD_DIR / group
    if not group_dir.exists():
        return
    
    logger.info(f"[ì •ë¦¬] '{group}' ê·¸ë£¹ ë””ë ‰í† ë¦¬ ë‚´ ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ ì¤‘...")
    for file_path in group_dir.glob(f"{group}_*.html"):
        if today_str not in file_path.name:
            try:
                file_path.unlink()
                logger.info(f"ğŸ—‘ï¸ ì‚­ì œë¨: {file_path}")
            except Exception as e:
                logger.warning(f"ì‚­ì œ ì‹¤íŒ¨: {file_path} | ì´ìœ : {e}")


def sanitize_filename(filename_component):
    """íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ì•ˆì „í•œ ë¬¸ìì—´ë¡œ ë³€í™˜"""
    return re.sub(r'[^\w\-_\.]', '_', str(filename_component))


def get_normalized_group_name(entry):
    """entryì—ì„œ groupì„ ì•ˆì „í•˜ê²Œ ì •ê·œí™”í•˜ì—¬ ë°˜í™˜"""
    raw_group = entry.get("group", "").strip().lower()
    return sanitize_filename(raw_group if raw_group else "unknown_group")


def wait_for_tor_proxy_ready(max_retries=10, delay_sec=10):
    logger.info("Tor í”„ë¡ì‹œ ì¤€ë¹„ ìƒíƒœ í™•ì¸ ì‹œì‘...")
    for attempt in range(1, max_retries + 1):
        logger.info(f"[Tor Check] ({attempt}/{max_retries})")
        try:
            proxies = {
                'http': f'socks5h://{TOR_PROXY_ADDRESS}',
                'https': f'socks5h://{TOR_PROXY_ADDRESS}'
            }
            res = requests.get('https://check.torproject.org/', proxies=proxies, timeout=5)
            if res.status_code == 200 and "Congratulations" in res.text:
                logger.info("[âœ“] Tor Proxy Ready!")
                return True
        except Exception as e:
            logger.warning(f"[Waiting for Tor Proxy...] ì˜¤ë¥˜: {e}")
        time.sleep(delay_sec)
    logger.error("[âœ–] Tor Proxy not ready after retries. Exiting.")
    return False

def fetch_url_with_curl(url, output_filepath):
    """ì£¼ì–´ì§„ URLì„ curlì„ ì‚¬ìš©í•´ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    logger.info(f"ğŸ‘‰ ìš”ì²­: {url}")
    curl_cmd = [
        "curl",
        "--fail", # HTTP ì˜¤ë¥˜ ì‹œ 0ì´ ì•„ë‹Œ ì¢…ë£Œ ì½”ë“œ ë°˜í™˜
        "--socks5-hostname", TOR_PROXY_ADDRESS,
        "-A", USER_AGENT, # User-Agent ì¶”ê°€
        "--connect-timeout", str(CURL_CONNECT_TIMEOUT),
        "--max-time", str(CURL_MAX_TIME),
        "-L", # ë¦¬ë‹¤ì´ë ‰ì…˜ ë”°ë¥´ê¸°
        url,
        "-o", str(output_filepath) # Path ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    ]

    try:
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± (ì—†ìœ¼ë©´)
        output_filepath.parent.mkdir(parents=True, exist_ok=True)
        
        process = subprocess.run(curl_cmd, check=True, capture_output=True, text=True, timeout=SUBPROCESS_TIMEOUT)
        logger.info(f"ì €ì¥ë¨: {output_filepath}")
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"í˜ì´ì§€ ì—†ìŒ ë˜ëŠ” curl ì‹¤íŒ¨ (ì¢…ë£Œ ì½”ë“œ {e.returncode}): {url}")
        logger.debug(f"Curl stdout: {e.stdout}")
        logger.debug(f"Curl stderr: {e.stderr}")
        return False
    except subprocess.TimeoutExpired:
        logger.error(f"Curl ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼: {url}")
        return False
    except Exception as e:
        logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ ({url}): {e}")
        return False



def run_crawler():
    if not ONION_LIST_PATH.exists() or ONION_LIST_PATH.stat().st_size == 0:
        logger.error(f"{ONION_LIST_PATH} íŒŒì¼ì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    try:
        with open(ONION_LIST_PATH, "r") as f:
            fqdn_list = json.load(f)
    except json.JSONDecodeError:
        logger.error(f"{ONION_LIST_PATH} íŒŒì¼ì˜ JSON í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    except Exception as e:
        logger.error(f"{ONION_LIST_PATH} íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return


    if not fqdn_list:
        logger.error("FQDN ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    today_str = datetime.now().strftime("%Y%m%d")
    # today_str = (datetime.now() - timedelta(days=1)).strftime("%Y%m%d") # ì–´ì œ ë‚ ì§œ í…ŒìŠ¤íŠ¸ìš©

    special_groups = ["play", "blacksuit", "kairos"] # í˜ì´ì§€ë„¤ì´ì…˜ ë°©ì‹ì´ ë‹¤ë¥¸ ê·¸ë£¹

    total_fqdns = len(fqdn_list)

    all_groups = set(get_normalized_group_name(entry) for entry in fqdn_list)
    for group in all_groups:
        clean_old_group_files(group, today_str)

    for i, entry in enumerate(fqdn_list):
        fqdn = entry.get("fqdn")
        group = get_normalized_group_name(entry)

        if not fqdn:
            logger.warning(f"FQDN ì •ë³´ê°€ ì—†ëŠ” í•­ëª© ê±´ë„ˆëœ€: {entry}")
            continue

        logger.info(f"\n--- [{i+1}/{total_fqdns}] í¬ë¡¤ë§ ì‹œì‘: {fqdn} (group: {group}) ---")

        for page_num in range(1, MAX_PAGES_PER_FQDN + 1):
            # URL ë° íŒŒì¼ëª… suffix ìƒì„± ë¡œì§
            if group in special_groups:
                if group == "kairos":
                    url = f"http://{fqdn}/" if page_num == 1 else f"http://{fqdn}/?PAGEN_1={page_num}"
                else: # play, blacksuit ë“±
                    url = f"http://{fqdn}/" if page_num == 1 else f"http://{fqdn}/index.php?page={page_num}"
            else: # ì¼ë°˜ ê·¸ë£¹
                url = f"http://{fqdn}/" if page_num == 1 else f"http://{fqdn}/?page={page_num}"
            
            suffix = f"page{page_num}"
            filename = f"{group}_{today_str}_{suffix}.html"
            # ì¶œë ¥ íŒŒì¼ ê²½ë¡œë¥¼ BASE_DOWNLOAD_DIR ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
            outfile_path = BASE_DOWNLOAD_DIR / group / filename # ê·¸ë£¹ë³„ í•˜ìœ„ ë””ë ‰í† ë¦¬ ìƒì„±
            # outfile_path = BASE_DOWNLOAD_DIR / filename # ëª¨ë“  íŒŒì¼ì„ ë™ì¼ ë””ë ‰í† ë¦¬ì— ì €ì¥í•˜ê³  ì‹¶ë‹¤ë©´

            if not fetch_url_with_curl(url, outfile_path):
                logger.warning(f"í˜ì´ì§€ {page_num} ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨. ë‹¤ìŒ FQDNìœ¼ë¡œ ì´ë™: {fqdn}")
                break # í˜„ì¬ FQDNì˜ ë‹¤ìŒ í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘ë‹¨, ë‹¤ìŒ FQDNìœ¼ë¡œ ë„˜ì–´ê°

def main():
    if not wait_for_tor_proxy_ready():
        return # Tor í”„ë¡ì‹œ ì¤€ë¹„ ì•ˆë˜ë©´ ì¢…ë£Œ

    logger.info("\n=== curl í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹œì‘ ===")
    run_crawler()
    logger.info("=== ëª¨ë“  ì‘ì—… ì™„ë£Œ, curl-crawler ì¢…ë£Œí•©ë‹ˆë‹¤. ===")

if __name__ == "__main__":
    main()
