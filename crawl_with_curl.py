import json
import subprocess
import time
from pathlib import Path
import requests
from datetime import datetime, timedelta

def wait_for_tor_proxy_ready(max_retries=10, delay_sec=10):
    for attempt in range(1, max_retries + 1):
        try:
            print(f"[Tor Check] ({attempt}/{max_retries})")
            proxies = {
                'http': 'socks5h://127.0.0.1:9050',
                'https': 'socks5h://127.0.0.1:9050'
            }
            res = requests.get('https://check.torproject.org/', proxies=proxies, timeout=5)
            if res.status_code == 200 and "Congratulations" in res.text:
                print("[âœ“] Tor Proxy Ready!")
                return
        except Exception:
            print("[Waiting for Tor Proxy...]")
        time.sleep(delay_sec)
    print("[âœ–] Tor Proxy not ready after retries. Exiting.")
    exit(1)

def run_crawler():
    json_path = Path("/app/downloads/onion_list.json")

    if not json_path.exists() or json_path.stat().st_size == 0:
        print("âŒ onion_list.json íŒŒì¼ì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    with open(json_path, "r") as f:
        fqdn_list = json.load(f)

    if not fqdn_list:
        print("âŒ FQDN ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    today_str = datetime.now().strftime("%Y%m%d")
    # today_str = (datetime.now() - timedelta(days=1)).strftime("%Y%m%d")

    # ì—¬ê¸°ì— íŠ¹ìˆ˜í•œ URL íŒ¨í„´ì„ ê°€ì§€ëŠ” ê·¸ë£¹ë“¤ì„ ì¶”ê°€
    special_groups = ["play", "blacksuit", "kairos"]

    for entry in fqdn_list:
        fqdn = entry.get("fqdn")
        group = entry.get("group", "").lower().strip()  # group í•„ë“œê°€ ì—†ê±°ë‚˜ ëŒ€ì†Œë¬¸ì ëŒ€ë¹„

        if not fqdn:
            continue

        print(f"\ní¬ë¡¤ë§ ì‹œì‘: {fqdn} (group: {group})")

        for page_num in range(1, 6):
            if group in special_groups:
                if group == "kairos":
                    if page_num == 1:
                        url = f"http://{fqdn}/"
                        suffix = "page1"
                    else:
                        url = f"http://{fqdn}/?PAGEN_1={page_num}"
                        suffix = f"page{page_num}"
                else:
                    if page_num == 1:
                        url = f"http://{fqdn}/"
                        suffix = "page1"
                    else:
                        url = f"http://{fqdn}/index.php?page={page_num}"
                        suffix = f"page{page_num}"
            else:
                if page_num == 1:
                    url = f"http://{fqdn}/"
                    suffix = "page1"
                else:
                    url = f"http://{fqdn}/?page={page_num}"
                    suffix = f"page{page_num}"


            filename = f"{group}_{today_str}_{suffix}.html"
            outfile = f"/app/downloads/{filename}"

            print(f"ğŸ‘‰ ìš”ì²­: {url}")
            curl_cmd = [
                "curl",
                "--fail",
                "--socks5-hostname", "127.0.0.1:9050",
                "--connect-timeout", "10",
                "--max-time", "30",
                "-L",
                url,
                "-o", outfile
            ]

            try:
                subprocess.run(curl_cmd, check=True)
                print(f"âœ… ì €ì¥ë¨: {outfile}")
            except subprocess.CalledProcessError:
                print(f"âŒ í˜ì´ì§€ ì—†ìŒ ë˜ëŠ” ì‹¤íŒ¨: {url} â†’ ë‹¤ìŒ fqdnìœ¼ë¡œ ì´ë™")
                break  # ë‹¤ìŒ fqdnìœ¼ë¡œ

def main():
    wait_for_tor_proxy_ready()
    print("\ncurl í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹œì‘")
    run_crawler()
    print("ëª¨ë“  ì‘ì—… ì™„ë£Œ, curl-crawler ì¢…ë£Œí•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
