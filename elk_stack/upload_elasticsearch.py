import glob
import os
from datetime import datetime

from elasticsearch import Elasticsearch

# Docker ê¸°ë°˜ Elasticsearch ì ‘ì† ì„¤ì •
es = Elasticsearch("http://localhost:9200", basic_auth=("elastic", "password"))

def parse_txt_article(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    title = lines[0].replace("ì œëª©: ", "").strip() if len(lines) > 0 else ""
    url = lines[1].replace("URL: ", "").strip() if len(lines) > 1 else ""
    content = "".join(lines[3:]).strip() if len(lines) > 3 else ""

    return {
        "title": title,
        "url": url,
        "published_at": datetime.today().strftime("%Y-%m-%d"),
        "source": "GBHackers",
        "tags": [],
        "summary": content[:300],
        "content": content
    }

def upload_articles_to_es(base_dir="downloads"):
    txt_files = glob.glob(os.path.join(base_dir, "**", "*.txt"), recursive=True)
    print(f"ì´ {len(txt_files)}ê°œì˜ ê¸°ì‚¬ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n")

    for file_path in txt_files:
        article_doc = parse_txt_article(file_path)

        if not article_doc["title"] or not article_doc["url"]:
            print(f"ìŠ¤í‚µë¨ (í•„ìˆ˜ ì •ë³´ ì—†ìŒ): {file_path}")
            continue

        res = es.index(index="security_news", document=article_doc)
        print(f"ì—…ë¡œë“œ ì™„ë£Œ: {article_doc['title']}")
        print(f"   ğŸ“ {article_doc['url']}\n")

if __name__ == "__main__":
    upload_articles_to_es()